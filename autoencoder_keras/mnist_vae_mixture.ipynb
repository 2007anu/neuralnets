{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Activation, Lambda, Embedding, Reshape, RepeatVector\n",
    "from keras.layers import merge\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import backend as K\n",
    "from keras import objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References :\n",
    "##### VAE with GMM : https://arxiv.org/pdf/1611.05148.pdf\n",
    "##### Categorical VAE parameterization : http://blog.evjang.com/2016/11/tutorial-categorical-variational.html\n",
    "##### Auto-Encoding Variational Bayes : https://arxiv.org/abs/1312.6114\n",
    "##### Tutorial from Oliver Durr : https://home.zhaw.ch/~dueo/bbs/files/vae.pdf\n",
    "##### Building autoencoders in keras : https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL between 2 normal distributions\n",
    "\n",
    "* a) if N(0,1)^d if one of the 2 distributions\n",
    "\n",
    "       KL = 0.5 SUM_OVER_DIMS[ -1 - log_var + square(mean) + var ]\n",
    "\n",
    "* b) for 2 diagonal covariances matrices\n",
    "\n",
    "       KL = 0.5 SUM_OVER_DIMS[ -1 + log(var2)-log(var1) + var1/var2 + (mean1-mean2)^2 / var2 ]\n",
    "       \n",
    "* c) see most general formula at :\n",
    "\n",
    "       stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians/60699#60699"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Variational Auto-Encoder with Gaussian Mixture generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "original_dim = 784\n",
    "latent_dim = 2\n",
    "\n",
    "NM = 10 # number of mixture clusters\n",
    "\n",
    "intermediate_dim1 = 500\n",
    "intermediate_dim2 = 500\n",
    "\n",
    "nb_epoch = 25\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparameterization trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Approximate Multinomial distrib. with a Gumbel-Softmax distrib.\n",
    "# see documentation at :\n",
    "# http://blog.evjang.com/2016/11/tutorial-categorical-variational.html\n",
    "def sampling_categorical(c_logits):\n",
    "    temperature = 0.1 # the lower it is the sharper the function is\n",
    "    eps = 1e-20\n",
    "    U = K.random_uniform(shape=(NM,), low=0, high=1)\n",
    "    gumbel_noise = -K.log(-K.log(U + eps) + eps)\n",
    "    # WARNING : dont return softmax as we apply it NEXT\n",
    "    return (c_logits + gumbel_noise) / temperature\n",
    "    #return K.softmax((c_logits+gumbel_noise)/temperature)\n",
    "\n",
    "def sampling_normal(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_pdf_normal(args):\n",
    "    # WARNING : assumes covariance is diagonal\n",
    "    # INFO : returns log of pdf for softmax function\n",
    "    mu, log_var, z = args\n",
    "    return -0.5 * ( K.sum((mu - z) * (mu - z) / K.exp(log_var))\n",
    "                    + K.sum(log_var)\n",
    "                    + latent_dim * math.log(2*math.pi) )\n",
    "\n",
    "def kl_normal(args):\n",
    "    i_mean, i_log_var, z_mean, z_log_var = args\n",
    "    return -0.5 * K.sum( 1 \n",
    "                        - i_log_var + z_log_var\n",
    "                        - K.exp(z_log_var)/K.exp(i_log_var)\n",
    "                        - K.square(z_mean - i_mean) / K.exp(i_log_var) , axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npredmat = Reshape((NM,original_dim))( merge(predictions, mode=\\'concat\\', concat_axis=1) ) #.summary to check axis\\n\\ndeltas = merge([RepeatVector(NM)(outputs), predmat], output_shape=(NM,original_dim), mode=lambda x: -(x[0] * K.log(x[1])))\\n\\ndeltasums = Lambda(lambda x: K.sum(x, axis=2), output_shape=lambda s: (s[0], s[1]))(deltas)# .summary to check axis\\n\\nhinton_trick = True # see \"Adaptive Mixtures of Local Experts\"\\nif hinton_trick:\\n    Hinton1 = Lambda(lambda x: K.exp(-x), output_shape=lambda s: s)\\n    deltasums = Hinton1(deltasums)\\n\\n# WARNING : gate is just the sampled c vector\\ngate = c\\n\\nerrors = merge([gate, deltasums], mode=\\'dot\\')\\n##############################\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h1 = Dense(intermediate_dim1, activation='softplus')(x)\n",
    "h2 = Dense(intermediate_dim2, activation='softplus')(h1)\n",
    "z_mean = Dense(latent_dim, activation=None)(h2)\n",
    "z_log_var = Dense(latent_dim, activation=None)(h2)\n",
    "\n",
    "#ONEINT = Input(batch_shape=(batch_size, 1))\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling_normal, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "E_mean = Embedding(NM, latent_dim, input_length=1) # WARNING : needs near-0 initialization\n",
    "E_log_var = Embedding(NM, latent_dim, input_length=1) # WARNING : needs near-1 initialization\n",
    "\n",
    "logpdfs = []\n",
    "kl_losses = []\n",
    "for i in range(NM):\n",
    "    # Tensor with constant i value\n",
    "    dummy_d = Dense(1, weights=[np.zeros((original_dim, 1)),i*np.ones((1,))], input_dim=original_dim)\n",
    "    dummy_d.trainable = False\n",
    "    ohe_i = dummy_d(x)\n",
    "    # Apply tensor to Mean and Var embeddings\n",
    "    i_mean = Reshape((latent_dim,))( E_mean(ohe_i) )\n",
    "    i_log_var = Reshape((latent_dim,))( E_log_var(ohe_i) )\n",
    "    # PDF\n",
    "    logpdfs.append( Reshape((1,))( Lambda(log_pdf_normal, output_shape=(1,))([i_mean, i_log_var, z]) ) )\n",
    "    # Compute KL-loss terms\n",
    "    kl_losses.append( Reshape((1,))( Lambda(kl_normal, output_shape=(1,))([i_mean, i_log_var, z_mean, z_log_var]) ) )\n",
    "\n",
    "\n",
    "#logpdfmat = Reshape((NM,1))( merge(logpdfs, mode='concat', concat_axis=1) ) # .summary to check axis\n",
    "logpdfmat = merge(logpdfs, mode='concat', concat_axis=1) # .summary to check axis\n",
    "c_logits = Activation('softmax')(logpdfmat)\n",
    "# WARNING : need to compute c_logits from z\n",
    "c = Activation('softmax')( Lambda(sampling_categorical, output_shape=(NM,))(c_logits) )\n",
    "\n",
    "klmat = Reshape((NM,))( merge(kl_losses, mode='concat', concat_axis=1) )\n",
    "\n",
    "kl_loss = merge([c, klmat], mode='dot') # WARNING : needs to be-log(SUM(prob_j * exp(-KL_j)))\n",
    "\n",
    "##############################\n",
    "\n",
    "#inputs = Input(shape=(nn_input_dim,))\n",
    "inputs = x\n",
    "#outputs = Input(shape=(8,)) # in AE model output=input=x\n",
    "outputs = x\n",
    "\n",
    "predictions = []\n",
    "for i in range(NM):\n",
    "    decoder_h1 = Dense(intermediate_dim1, activation='softplus')\n",
    "    decoder_h2 = Dense(intermediate_dim2, activation='softplus')\n",
    "    decoder_proba = Dense(original_dim, activation='sigmoid')\n",
    "    h1_decoded = decoder_h1(z)\n",
    "    h2_decoded = decoder_h2(h1_decoded)\n",
    "    i_x_decoded_proba = decoder_proba(h2_decoded) # a Bernoulli dist. has a single prob. parameter\n",
    "    predictions.append( i_x_decoded_proba )\n",
    "\n",
    "\n",
    "x_decoded_proba = predictions[0]\n",
    "\n",
    "'''\n",
    "predmat = Reshape((NM,original_dim))( merge(predictions, mode='concat', concat_axis=1) ) #.summary to check axis\n",
    "\n",
    "deltas = merge([RepeatVector(NM)(outputs), predmat], output_shape=(NM,original_dim), mode=lambda x: -(x[0] * K.log(x[1])))\n",
    "\n",
    "deltasums = Lambda(lambda x: K.sum(x, axis=2), output_shape=lambda s: (s[0], s[1]))(deltas)# .summary to check axis\n",
    "\n",
    "hinton_trick = True # see \"Adaptive Mixtures of Local Experts\"\n",
    "if hinton_trick:\n",
    "    Hinton1 = Lambda(lambda x: K.exp(-x), output_shape=lambda s: s)\n",
    "    deltasums = Hinton1(deltasums)\n",
    "\n",
    "# WARNING : gate is just the sampled c vector\n",
    "gate = c\n",
    "\n",
    "errors = merge([gate, deltasums], mode='dot')\n",
    "##############################\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_proba):\n",
    "    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_proba)\n",
    "    #kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss# + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_proba)\n",
    "vae.compile(optimizer=Adam(1e-3), loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mdl = Model([x, ONEINT], [logpdfs[0], logpdfs[1], logpdfs[2], logpdfs[3], logpdfs[4]])\n",
    "#mdl = Model([x, ONEINT], logpdfmat)\n",
    "#mdl = Model([x, ONEINT], c_logits)\n",
    "#mdl = Model(x, c)\n",
    "#mdl = Model(x, kl_losses[0])\n",
    "#mdl = Model(x, klmat)\n",
    "mdl = Model(x, kl_loss)\n",
    "#mdl = Model([x, ONEINT], predmat)\n",
    "#mdl = Model([x, ONEINT], deltas)\n",
    "#mdl = Model([x, ONEINT], deltasums)\n",
    "#mdl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 10.58854771],\n",
       "       [ 10.58854771],\n",
       "       [ 10.58854771]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#onearr = np.ones((batch_size, 1)).astype(int)\n",
    "print mdl.predict(x_train[0:batch_size,:]).shape\n",
    "np.array(mdl.predict(x_train[0:batch_size,:]))[0:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  3.01692512e-06,   2.89611307e-06,   5.49303468e-06, ...,\n",
       "          2.95742734e-06,   1.25978045e-06,   1.98960765e-06],\n",
       "       [  7.53871234e-07,   2.33677406e-06,   8.20605965e-06, ...,\n",
       "          9.93965955e-07,   4.22242283e-06,   2.02209208e-06],\n",
       "       [  1.51504019e-05,   1.18276748e-05,   3.19873943e-05, ...,\n",
       "          3.02769040e-05,   2.15850432e-05,   3.32489908e-05]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print vae.predict(x_train[0:batch_size,:]).shape\n",
    "np.array(vae.predict(x_train[0:batch_size,:]))[0:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 26s - loss: 156.9996    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe19a54f8d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=1, # may have to increase this value\n",
    "        batch_size=batch_size)\n",
    "        #validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder_mean = Model(x, z_mean)\n",
    "#encoder_stdev = Model(x, K.exp(z_log_var / 2))\n",
    "\n",
    "#encoder = Model(x, z)\n",
    "\n",
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h1_decoded = decoder_h1(decoder_input)\n",
    "_h2_decoded = decoder_h2(_h1_decoded)\n",
    "_x_decoded_proba = decoder_proba(_h2_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
