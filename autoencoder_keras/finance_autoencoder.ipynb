{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference :\n",
    "* Blog : building autoencoders in keras : https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load market data from Quandl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import quandl # pip install quandl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qData(tick='XLU'):\n",
    "    # GOOG/NYSE_XLU.4\n",
    "    # WIKI/MSFT.4\n",
    "    qtck = \"GOOG/NYSE_\"+tick+\".4\"\n",
    "    return quandl.get(qtck,\n",
    "                      start_date=\"2003-01-01\",\n",
    "                      end_date=\"2016-12-31\",\n",
    "                      collapse=\"daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''TICKERS = ['MSFT','JPM','INTC','DOW','KO',\n",
    "             'MCD','CAT','WMT','MMM','AXP',\n",
    "             'BA','GE','XOM','PG','JNJ']'''\n",
    "TICKERS = ['XLU','XLF','XLK','XLY','XLV','XLB','XLE','XLP','XLI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    D.keys()\n",
    "except:\n",
    "    print('create empty Quandl cache')\n",
    "    D = {}\n",
    "\n",
    "for tckr in TICKERS:\n",
    "    if not(tckr in D.keys()):\n",
    "        print(tckr)\n",
    "        qdt = qData(tckr)\n",
    "        qdt.rename(columns={'Close': tckr}, inplace = True)\n",
    "        D[tckr] = qdt\n",
    "        \n",
    "for tck in D.keys():\n",
    "    assert(D[tck].keys() == [tck])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3538, 1)\n",
      "(3538, 1)\n",
      "(3538, 1)\n",
      "(3538, 1)\n",
      "(3538, 1)\n",
      "(3538, 1)\n",
      "(3538, 1)\n",
      "(3538, 1)\n",
      "(3538, 1)\n"
     ]
    }
   ],
   "source": [
    "for tck in D.keys():\n",
    "    print(D[tck].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "J = D[TICKERS[0]].join(D[TICKERS[1]])\n",
    "for tck in TICKERS[2:]:\n",
    "    J = J.join(D[tck])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>XLU</th>\n",
       "      <th>XLF</th>\n",
       "      <th>XLK</th>\n",
       "      <th>XLY</th>\n",
       "      <th>XLV</th>\n",
       "      <th>XLB</th>\n",
       "      <th>XLE</th>\n",
       "      <th>XLP</th>\n",
       "      <th>XLI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-01-02</th>\n",
       "      <td>19.60</td>\n",
       "      <td>22.80</td>\n",
       "      <td>15.60</td>\n",
       "      <td>23.98</td>\n",
       "      <td>27.27</td>\n",
       "      <td>20.36</td>\n",
       "      <td>22.80</td>\n",
       "      <td>20.32</td>\n",
       "      <td>21.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-03</th>\n",
       "      <td>19.80</td>\n",
       "      <td>22.78</td>\n",
       "      <td>15.66</td>\n",
       "      <td>23.43</td>\n",
       "      <td>27.55</td>\n",
       "      <td>20.25</td>\n",
       "      <td>22.76</td>\n",
       "      <td>20.30</td>\n",
       "      <td>21.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-06</th>\n",
       "      <td>20.69</td>\n",
       "      <td>23.55</td>\n",
       "      <td>16.35</td>\n",
       "      <td>23.86</td>\n",
       "      <td>27.85</td>\n",
       "      <td>20.64</td>\n",
       "      <td>22.95</td>\n",
       "      <td>20.45</td>\n",
       "      <td>21.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-07</th>\n",
       "      <td>20.24</td>\n",
       "      <td>23.29</td>\n",
       "      <td>16.52</td>\n",
       "      <td>23.73</td>\n",
       "      <td>27.45</td>\n",
       "      <td>20.57</td>\n",
       "      <td>22.20</td>\n",
       "      <td>20.27</td>\n",
       "      <td>21.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-08</th>\n",
       "      <td>20.38</td>\n",
       "      <td>23.05</td>\n",
       "      <td>15.98</td>\n",
       "      <td>23.51</td>\n",
       "      <td>27.24</td>\n",
       "      <td>20.04</td>\n",
       "      <td>21.99</td>\n",
       "      <td>20.15</td>\n",
       "      <td>21.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              XLU    XLF    XLK    XLY    XLV    XLB    XLE    XLP    XLI\n",
       "Date                                                                     \n",
       "2003-01-02  19.60  22.80  15.60  23.98  27.27  20.36  22.80  20.32  21.12\n",
       "2003-01-03  19.80  22.78  15.66  23.43  27.55  20.25  22.76  20.30  21.19\n",
       "2003-01-06  20.69  23.55  16.35  23.86  27.85  20.64  22.95  20.45  21.38\n",
       "2003-01-07  20.24  23.29  16.52  23.73  27.45  20.57  22.20  20.27  21.26\n",
       "2003-01-08  20.38  23.05  15.98  23.51  27.24  20.04  21.99  20.15  21.00"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLU    0\n",
       "XLF    0\n",
       "XLK    0\n",
       "XLY    0\n",
       "XLV    0\n",
       "XLB    0\n",
       "XLE    0\n",
       "XLP    0\n",
       "XLI    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3537, 9)\n",
      "(3537, 9)\n"
     ]
    }
   ],
   "source": [
    "J2 = J.fillna(method='ffill')\n",
    "#J2[J['WMT'].isnull()]\n",
    "\n",
    "LogDiffJ = J2.apply(np.log).diff(periods=1, axis=0)\n",
    "LogDiffJ.drop(LogDiffJ.index[0:1], inplace=True)\n",
    "print LogDiffJ.shape\n",
    "\n",
    "MktData = LogDiffJ.as_matrix(columns=None) # as numpy.array\n",
    "print MktData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(MktData)\n",
    "split_index = 3000\n",
    "x_train = MktData[0:split_index,:]*100\n",
    "x_test = MktData[split_index:,:]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0982247 ,  2.03701577,  1.29596141,  1.32517727,  1.04548284,\n",
       "        1.53840115,  1.77989192,  0.83855434,  1.3078465 ])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(x_train, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear auto-encoder : like PCA\n",
    "### We get a linear model by removing activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_dim = 9\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 3\n",
    "\n",
    "# this is our input placeholder\n",
    "input_data = Input(shape=(original_dim,))\n",
    "\n",
    "if True: # no sparsity constraint\n",
    "    encoded = Dense(encoding_dim, activation=None)(input_data)\n",
    "else:\n",
    "    encoded = Dense(encoding_dim, activation=None,\n",
    "                    activity_regularizer=regularizers.activity_l1(10e-5))(input_data)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(original_dim, activation=None)(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(inputs=input_data, outputs=decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(inputs=input_data, outputs=encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train autoencoder to reconstruct Stock returns\n",
    "# use L2 loss\n",
    "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 537 samples\n",
      "Epoch 1/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2280\n",
      "Epoch 2/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2340 - val_loss: 0.2279\n",
      "Epoch 3/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2280\n",
      "Epoch 4/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2280\n",
      "Epoch 5/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2340 - val_loss: 0.2279\n",
      "Epoch 6/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2340 - val_loss: 0.2279\n",
      "Epoch 7/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2340 - val_loss: 0.2280\n",
      "Epoch 8/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2280\n",
      "Epoch 9/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2280\n",
      "Epoch 10/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2280\n",
      "Epoch 11/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 12/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 13/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2340 - val_loss: 0.2279\n",
      "Epoch 14/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 15/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2280\n",
      "Epoch 16/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 17/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2340 - val_loss: 0.2279\n",
      "Epoch 18/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 19/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 20/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2280\n",
      "Epoch 21/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 22/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2280\n",
      "Epoch 23/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 24/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 25/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 26/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 27/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 28/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 29/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 30/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 31/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 32/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 33/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 34/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2277\n",
      "Epoch 35/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2277\n",
      "Epoch 36/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 37/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2279\n",
      "Epoch 38/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 39/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2277\n",
      "Epoch 40/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 41/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2277\n",
      "Epoch 42/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2277\n",
      "Epoch 43/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2277\n",
      "Epoch 44/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 45/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 46/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 47/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 48/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n",
      "Epoch 49/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2338 - val_loss: 0.2277\n",
      "Epoch 50/50\n",
      "3000/3000 [==============================] - 0s - loss: 0.2339 - val_loss: 0.2278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17f8af4590>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_data = encoder.predict(x_test)\n",
    "decoded_data = decoder.predict(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.769975033646\n",
      "1 0.99518747652\n",
      "2 0.901590510518\n",
      "3 0.947480846807\n",
      "4 0.82774934141\n",
      "5 0.900718119215\n",
      "6 0.989944764408\n",
      "7 0.854005961685\n",
      "8 0.930788316892\n"
     ]
    }
   ],
   "source": [
    "for i in range(original_dim):\n",
    "    print i, np.corrcoef(x_test[:,i].T, decoded_data[:,i].T)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.11938556286\n",
      "1 -0.115034789311\n",
      "2 -0.0466444153257\n",
      "3 0.241456431721\n",
      "4 -0.140337077375\n",
      "5 -0.100318098446\n",
      "6 0.106878897453\n",
      "7 0.0370390714887\n",
      "8 -0.0743364310779\n"
     ]
    }
   ],
   "source": [
    "decoding_error = x_test - decoded_data\n",
    "for i in range(original_dim):\n",
    "    print i, np.corrcoef(decoded_data[:,i].T, decoding_error[:,i].T)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
