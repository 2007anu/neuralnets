{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Rather than importing everything manually, we'll make things easy\n",
    "#   and load them all in utils.py, and just import them from there.\n",
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division,print_function\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import utils; reload(utils)\n",
    "from utils import plots, get_batches, plot_confusion_matrix, get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import random, permutation\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#path = \"../data/dogsandcats_small/\" # we copied a fraction of the full set for tests\n",
    "path = \"../data/dogsandcats/\"\n",
    "model_path = path + \"models/\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, \n",
    "                batch_size=batch_size, class_mode='categorical'):\n",
    "    return gen.flow_from_directory(path+dirname, target_size=(224,224), \n",
    "                class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 2 classes.\n",
      "Found 21000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Use batch size of 1 since we're just doing preprocessing on the CPU\n",
    "val_batches = get_batches('valid', shuffle=False, batch_size=batch_size) # no shuffle as we store conv output\n",
    "trn_batches = get_batches('train', shuffle=False, batch_size=batch_size) # no shuffle as we store conv output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat/cat.1262.jpg',\n",
       " 'cat/cat.9495.jpg',\n",
       " 'cat/cat.3044.jpg',\n",
       " 'cat/cat.1424.jpg',\n",
       " 'cat/cat.8210.jpg',\n",
       " 'cat/cat.8847.jpg',\n",
       " 'cat/cat.308.jpg',\n",
       " 'cat/cat.10802.jpg',\n",
       " 'cat/cat.5060.jpg',\n",
       " 'cat/cat.10406.jpg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_batches.filenames[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_labels = onehot(val_batches.classes)\n",
    "trn_labels = onehot(trn_batches.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''try:\n",
    "    trn = load_array(model_path+'train_data.bc')\n",
    "except:\n",
    "    trn = get_data(path+'train')\n",
    "    save_array(model_path+'train_data.bc', trn)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''try:\n",
    "    val = load_array(model_path+'valid_data.bc')\n",
    "except:\n",
    "    val = get_data(path+'valid')\n",
    "    save_array(model_path+'valid_data.bc', val)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''gen = image.ImageDataGenerator(rotation_range=10, width_shift_range=0.05, \n",
    "                               zoom_range=0.05,\n",
    "                               #channel_shift_range=10,\n",
    "                               height_shift_range=0.05, shear_range=0.05, horizontal_flip=False)\n",
    "trn_batchesRND = gen.flow(trn, trn_labels, batch_size=batch_size)\n",
    "val_batchesRND = gen.flow(val, val_labels, batch_size=batch_size)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/layers/core.py:622: UserWarning: `output_shape` argument not specified for layer lambda_2 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 3, 224, 224)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    realvgg = Vgg16()\n",
    "    conv_layers, fc_layers = split_at(realvgg.model, Flatten)\n",
    "    #conv_layers, fc_layers = split_at(realvgg.model, Convolution2D)\n",
    "    conv_model = Sequential(conv_layers)\n",
    "    conv_model_hash = 'conv_v3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will take a few minutes to complete the 1st time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    try:\n",
    "        val_convfeatures = load_array(model_path+'valid_'+conv_model_hash+'_features.bc')\n",
    "        if False: # force update\n",
    "            raise\n",
    "    except:\n",
    "        print('Missing file')\n",
    "        val_convfeatures = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "        save_array(model_path+'valid_'+conv_model_hash+'_features.bc', val_convfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will take a few minutes (maybe 10) to complete the 1st time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    try:\n",
    "        trn_convfeatures = load_array(model_path+'train_'+conv_model_hash+'_features.bc')\n",
    "        if False: # force update\n",
    "            raise\n",
    "    except:\n",
    "        print('Missing file')\n",
    "        trn_convfeatures = conv_model.predict_generator(trn_batches, trn_batches.nb_sample)\n",
    "        save_array(model_path+'train_'+conv_model_hash+'_features.bc', trn_convfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ready to train the model\n",
    "#### We use VGG top layers but we insert BatchNorm layers\n",
    "#### BatchNorm layers needs to be initialized properly so we first estimate\n",
    "#### the mean/var of the layers feeding into them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see : https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson3.ipynb\n",
    "\n",
    "def proc_wgts(layer, ndo):\n",
    "    # copy the weights from the pre-trained model\n",
    "    # original weights are for a 50% drop out\n",
    "    # we infer the corresponding weight for a new drop out (ndo) level\n",
    "    return [w*0.5/(1.-ndo) for w in layer.get_weights()]\n",
    "\n",
    "def get_fc_model(ndo):\n",
    "    model = Sequential([\n",
    "        Dense(4096, activation='relu', input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Dropout(ndo),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(ndo),\n",
    "        Dense(2, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    for l_new, l_orig in zip(model.layers[0:3], fc_layers[0:3]):\n",
    "        assert (type(l_new) == type(l_orig))\n",
    "        l_new.set_weights(proc_wgts(l_orig, ndo))\n",
    "    \n",
    "    for layer in model.layers[:-1]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    model.layers[-1].trainable = True\n",
    "    \n",
    "    #opt = RMSprop(lr=0.00001, rho=0.7)\n",
    "    opt = Adam()\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_model(p):\n",
    "    dense_model =  get_fc_model(p)\n",
    "\n",
    "    k_layer_out0 = K.function([dense_model.layers[0].input, K.learning_phase()],\n",
    "                              [dense_model.layers[0].output])\n",
    "    d0_out = k_layer_out0([trn_convfeatures, 0])[0]\n",
    "    mu0, var0 = d0_out.mean(axis=0), d0_out.var(axis=0)\n",
    "\n",
    "\n",
    "    k_layer_out2 = K.function([dense_model.layers[0].input, K.learning_phase()],\n",
    "                              [dense_model.layers[2].output])\n",
    "    d2_out = k_layer_out2([trn_convfeatures, 0])[0]\n",
    "    mu2, var2 = d2_out.mean(axis=0), d2_out.var(axis=0)\n",
    "\n",
    "    bn_model = insert_layer(dense_model, BatchNormalization(), 1)\n",
    "    bn_model = insert_layer(bn_model, BatchNormalization(), 4) # shifted due to insertion\n",
    "\n",
    "    bnl1 = bn_model.layers[1]\n",
    "    bnl4 = bn_model.layers[4]\n",
    "\n",
    "    #After inserting the layers, we can set their weights to the variance and mean we just calculated.\n",
    "    bnl1.set_weights([var0, mu0, mu0, var0])\n",
    "    bnl4.set_weights([var2, mu2, mu2, var2])\n",
    "\n",
    "    bn_model.compile(Adam(1e-3), 'categorical_crossentropy', ['accuracy'])\n",
    "    \n",
    "    for layer in bn_model.layers:\n",
    "        layer.trainable = False\n",
    "    bn_model.layers[-1].trainable = True\n",
    "    \n",
    "    return bn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_fresh_bn(mdl, top=2, full=5):\n",
    "    # top\n",
    "    for layer in mdl.layers:\n",
    "        layer.trainable = False\n",
    "    mdl.layers[-1].trainable = True\n",
    "    mdl.optimizer.lr = 1e-3\n",
    "    mdl.fit(trn_convfeatures, trn_labels, validation_data=(val_convfeatures, val_labels), nb_epoch=top)\n",
    "    # full\n",
    "    for layer in mdl.layers:\n",
    "        layer.trainable = True\n",
    "    mdl.optimizer.lr = 0.01*1e-3\n",
    "    mdl.fit(trn_convfeatures, trn_labels, validation_data=(val_convfeatures, val_labels), nb_epoch=full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bn_model = get_bn_model(0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_fresh_bn(bn_model, 2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train one or several models (ensembling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.1018 - acc: 0.9642 - val_loss: 0.0605 - val_acc: 0.9805\n",
      "Epoch 2/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0902 - acc: 0.9708 - val_loss: 0.0663 - val_acc: 0.9798\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0998 - acc: 0.9685 - val_loss: 0.0456 - val_acc: 0.9828\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0870 - acc: 0.9715 - val_loss: 0.0576 - val_acc: 0.9818\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0747 - acc: 0.9751 - val_loss: 0.0516 - val_acc: 0.9815\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0810 - acc: 0.9735 - val_loss: 0.0456 - val_acc: 0.9845\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0852 - acc: 0.9730 - val_loss: 0.0515 - val_acc: 0.9810\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.1048 - acc: 0.9648 - val_loss: 0.0449 - val_acc: 0.9840\n",
      "Epoch 2/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0898 - acc: 0.9708 - val_loss: 0.0597 - val_acc: 0.9762\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0903 - acc: 0.9701 - val_loss: 0.0465 - val_acc: 0.9840\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0882 - acc: 0.9712 - val_loss: 0.0453 - val_acc: 0.9838\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0826 - acc: 0.9728 - val_loss: 0.0520 - val_acc: 0.9822\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0868 - acc: 0.9721 - val_loss: 0.0610 - val_acc: 0.9792\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0947 - acc: 0.9703 - val_loss: 0.0677 - val_acc: 0.9792\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.1010 - acc: 0.9630 - val_loss: 0.0511 - val_acc: 0.9815\n",
      "Epoch 2/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0929 - acc: 0.9690 - val_loss: 0.0463 - val_acc: 0.9825\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0833 - acc: 0.9725 - val_loss: 0.0454 - val_acc: 0.9830\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0859 - acc: 0.9718 - val_loss: 0.0524 - val_acc: 0.9818\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0917 - acc: 0.9703 - val_loss: 0.0456 - val_acc: 0.9835\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0903 - acc: 0.9725 - val_loss: 0.0509 - val_acc: 0.9832\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0848 - acc: 0.9729 - val_loss: 0.0447 - val_acc: 0.9835\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.1083 - acc: 0.9622 - val_loss: 0.0476 - val_acc: 0.9822\n",
      "Epoch 2/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0991 - acc: 0.9678 - val_loss: 0.0425 - val_acc: 0.9845\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0919 - acc: 0.9704 - val_loss: 0.0507 - val_acc: 0.9832\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0827 - acc: 0.9729 - val_loss: 0.0472 - val_acc: 0.9842\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0885 - acc: 0.9713 - val_loss: 0.0502 - val_acc: 0.9810\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0754 - acc: 0.9746 - val_loss: 0.0520 - val_acc: 0.9805\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0755 - acc: 0.9754 - val_loss: 0.0470 - val_acc: 0.9820\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.1034 - acc: 0.9629 - val_loss: 0.0592 - val_acc: 0.9765\n",
      "Epoch 2/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0855 - acc: 0.9705 - val_loss: 0.0450 - val_acc: 0.9835\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0877 - acc: 0.9712 - val_loss: 0.0464 - val_acc: 0.9810\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0914 - acc: 0.9714 - val_loss: 0.0459 - val_acc: 0.9840\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0879 - acc: 0.9712 - val_loss: 0.0557 - val_acc: 0.9805\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0840 - acc: 0.9715 - val_loss: 0.0513 - val_acc: 0.9772\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0821 - acc: 0.9732 - val_loss: 0.0515 - val_acc: 0.9830\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0994 - acc: 0.9640 - val_loss: 0.0688 - val_acc: 0.9768\n",
      "Epoch 2/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0912 - acc: 0.9691 - val_loss: 0.0539 - val_acc: 0.9778\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0899 - acc: 0.9708 - val_loss: 0.0485 - val_acc: 0.9828\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0850 - acc: 0.9732 - val_loss: 0.0494 - val_acc: 0.9818\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0774 - acc: 0.9755 - val_loss: 0.0713 - val_acc: 0.9775\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0826 - acc: 0.9715 - val_loss: 0.0530 - val_acc: 0.9790\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0801 - acc: 0.9736 - val_loss: 0.0465 - val_acc: 0.9820\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.1125 - acc: 0.9622 - val_loss: 0.0463 - val_acc: 0.9820\n",
      "Epoch 2/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0960 - acc: 0.9678 - val_loss: 0.0528 - val_acc: 0.9840\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0886 - acc: 0.9693 - val_loss: 0.0424 - val_acc: 0.9845\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0836 - acc: 0.9723 - val_loss: 0.0436 - val_acc: 0.9852\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0801 - acc: 0.9733 - val_loss: 0.0497 - val_acc: 0.9795\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0883 - acc: 0.9719 - val_loss: 0.0595 - val_acc: 0.9810\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0890 - acc: 0.9725 - val_loss: 0.0481 - val_acc: 0.9802\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0994 - acc: 0.9650 - val_loss: 0.0934 - val_acc: 0.9663\n",
      "Epoch 2/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0967 - acc: 0.9683 - val_loss: 0.0469 - val_acc: 0.9832\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0821 - acc: 0.9737 - val_loss: 0.0485 - val_acc: 0.9842\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0818 - acc: 0.9730 - val_loss: 0.0527 - val_acc: 0.9812\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0936 - acc: 0.9710 - val_loss: 0.0558 - val_acc: 0.9828\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0776 - acc: 0.9746 - val_loss: 0.0553 - val_acc: 0.9825\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0866 - acc: 0.9731 - val_loss: 0.0699 - val_acc: 0.9780\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.1162 - acc: 0.9590 - val_loss: 0.0487 - val_acc: 0.9812\n",
      "Epoch 2/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0872 - acc: 0.9712 - val_loss: 0.0473 - val_acc: 0.9828\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0852 - acc: 0.9720 - val_loss: 0.0453 - val_acc: 0.9832\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0811 - acc: 0.9730 - val_loss: 0.0457 - val_acc: 0.9845\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0754 - acc: 0.9742 - val_loss: 0.0513 - val_acc: 0.9828\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0861 - acc: 0.9723 - val_loss: 0.0514 - val_acc: 0.9795\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0758 - acc: 0.9751 - val_loss: 0.0521 - val_acc: 0.9838\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0997 - acc: 0.9637 - val_loss: 0.0922 - val_acc: 0.9698\n",
      "Epoch 2/2\n",
      "21000/21000 [==============================] - 11s - loss: 0.0913 - acc: 0.9688 - val_loss: 0.0534 - val_acc: 0.9848\n",
      "Train on 21000 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0881 - acc: 0.9718 - val_loss: 0.0479 - val_acc: 0.9820\n",
      "Epoch 2/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0807 - acc: 0.9725 - val_loss: 0.0710 - val_acc: 0.9782\n",
      "Epoch 3/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0895 - acc: 0.9710 - val_loss: 0.0477 - val_acc: 0.9842\n",
      "Epoch 4/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0786 - acc: 0.9743 - val_loss: 0.0455 - val_acc: 0.9830\n",
      "Epoch 5/5\n",
      "21000/21000 [==============================] - 11s - loss: 0.0809 - acc: 0.9738 - val_loss: 0.0537 - val_acc: 0.9808\n"
     ]
    }
   ],
   "source": [
    "bn_models = []\n",
    "for i in range(10): # INFO : change here the size of the ensemble\n",
    "    bn_models.append( get_bn_model(0.30) )\n",
    "    train_fresh_bn(bn_models[-1], 2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''i = 0\n",
    "\n",
    "x_conv_model = Sequential(conv_layers)\n",
    "for layer in x_conv_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in ll_models[i].layers:\n",
    "    x_conv_model.add(layer)\n",
    "    \n",
    "#for l1,l2 in zip(conv_model.layers[last_conv_idx+1:], fc_model.layers): \n",
    "#        l1.set_weights(l2.get_weights())\n",
    "x_conv_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#x_conv_model.save_weights(model_path+'no_dropout_bn' + i + '.h5')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''for layer in x_conv_model.layers[-5:]:\n",
    "    layer.trainable = True\n",
    "x_conv_model.optimizer.lr = 1e-6'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 167s - loss: 0.0266 - acc: 0.9888 - val_loss: 0.0518 - val_acc: 0.9790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c6e529410>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''x_conv_model.fit_generator(trn_batchesRND,\n",
    "                           samples_per_epoch = min(40*batch_size,trn_batchesRND.n),\n",
    "                           nb_epoch = 1,\n",
    "                           validation_data = val_batchesRND,\n",
    "                           nb_val_samples = min(20*batch_size,val_batchesRND.n))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5\n",
      "-4\n",
      "-3\n",
      "-2\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "'''for mdl in ll_models:\n",
    "    for k in range(-len(mdl.layers),0):\n",
    "        print(k)\n",
    "        #x_conv_model.layers[k].get_weights()\n",
    "        #mdl.layers[k].set_weights\n",
    "        mdl.layers[k].set_weights( x_conv_model.layers[k].get_weights() )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.984499990940094, dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if False:\n",
    "    models = [bn_model] # without ensembling\n",
    "else:\n",
    "    models = bn_models # with ensembling\n",
    "\n",
    "all_val_preds = []\n",
    "for mdl in models:\n",
    "    these_val_preds = mdl.predict_on_batch(val_convfeatures)\n",
    "    assert(len(these_val_preds) == 4000)\n",
    "    all_val_preds.append( these_val_preds )\n",
    "mean_val_preds = np.stack(all_val_preds).mean(axis=0)\n",
    "categorical_accuracy(val_labels, mean_val_preds).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WARNING : should save each model of the ensemble\n",
    "#ll_model.save_weights(model_path+'llmodel_finetune1.h5')\n",
    "#ll_model.load_weights(model_path+'llmodel_finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['test/10592.jpg',\n",
       " 'test/7217.jpg',\n",
       " 'test/3653.jpg',\n",
       " 'test/4382.jpg',\n",
       " 'test/2924.jpg',\n",
       " 'test/10.jpg',\n",
       " 'test/10916.jpg',\n",
       " 'test/12374.jpg',\n",
       " 'test/1871.jpg',\n",
       " 'test/11645.jpg']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batches = get_batches('test', shuffle=False, batch_size=batch_size, class_mode=None)\n",
    "testfiles = test_batches.filenames\n",
    "testfiles[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will take a few minutes (maybe 5) to complete the 1st time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_convfeatures = load_array(model_path+'test_'+conv_model_hash+'_features.bc')\n",
    "    if False: # force update\n",
    "        raise\n",
    "except:\n",
    "    print('Missing file')\n",
    "    test_convfeatures = conv_model.predict_generator(test_batches, test_batches.nb_sample)\n",
    "    save_array(model_path+'test_'+conv_model_hash+'_features.bc', test_convfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    models = [bn_model] # without ensembling\n",
    "else:\n",
    "    models = bn_models # with ensembling\n",
    "\n",
    "all_test_preds = []\n",
    "for mdl in models:\n",
    "    these_test_preds = mdl.predict_on_batch(test_convfeatures)\n",
    "    assert(len(these_test_preds) == 12500)\n",
    "    all_test_preds.append( these_test_preds )\n",
    "mean_test_preds = np.stack(all_test_preds).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.9996e-01,   3.8756e-05],\n",
       "       [  9.9993e-01,   6.8629e-05],\n",
       "       [  2.0637e-04,   9.9979e-01],\n",
       "       [  9.9551e-01,   4.4935e-03],\n",
       "       [  1.4125e-02,   9.8587e-01],\n",
       "       [  1.0000e+00,   3.3480e-06],\n",
       "       [  2.2238e-03,   9.9778e-01],\n",
       "       [  1.0000e+00,   1.2753e-07],\n",
       "       [  1.8051e-04,   9.9982e-01],\n",
       "       [  6.9930e-05,   9.9993e-01]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_test_preds[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.018694336826138514, -0.018949097448258439)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_idx = 1\n",
    "eps = 1e-3 # WARNING : this has significant impact\n",
    "digits = 3 # WARNING : this has significant impact\n",
    "\n",
    "cut = lambda x : round(min(max(x,eps),1-eps),digits)\n",
    "\n",
    "a = sum([p[dog_idx]*math.log(p[dog_idx]) for p in mean_test_preds])/len(mean_test_preds)\n",
    "b = sum([p[dog_idx]*math.log(cut(p[dog_idx])) for p in mean_test_preds])/len(mean_test_preds)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1, 'label': 0.999},\n",
       " {'id': 2, 'label': 0.999},\n",
       " {'id': 3, 'label': 0.999},\n",
       " {'id': 4, 'label': 0.999},\n",
       " {'id': 5, 'label': 0.001},\n",
       " {'id': 6, 'label': 0.001},\n",
       " {'id': 7, 'label': 0.001},\n",
       " {'id': 8, 'label': 0.001},\n",
       " {'id': 9, 'label': 0.001},\n",
       " {'id': 10, 'label': 0.001},\n",
       " {'id': 11, 'label': 0.001},\n",
       " {'id': 12, 'label': 0.999},\n",
       " {'id': 13, 'label': 0.001},\n",
       " {'id': 14, 'label': 0.003},\n",
       " {'id': 15, 'label': 0.001},\n",
       " {'id': 16, 'label': 0.001},\n",
       " {'id': 17, 'label': 0.998},\n",
       " {'id': 18, 'label': 0.999}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = [{'id':int(f.split('/')[-1].split('.')[0]), 'label':cut(p[dog_idx])} for f, p in zip(testfiles, mean_test_preds)]\n",
    "def comp(x,y):\n",
    "    return int(x['id']) - int(y['id'])\n",
    "Z1 = sorted(Z1, comp)\n",
    "Z1[0:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('predictions_v4_9.csv', 'w') as csvfile:\n",
    "    fieldnames = ['id', 'label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for z in Z1:\n",
    "        writer.writerow(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
