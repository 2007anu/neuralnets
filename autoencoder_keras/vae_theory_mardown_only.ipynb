{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)\n",
    "A tutorial with code for a VAE as described in [Kingma and Welling, 2013](http://arxiv.org/abs/1312.6114). A talk with more details was given at the [DataLab Brown Bag Seminar](https://home.zhaw.ch/~dueo/bbs/files/vae.pdf).\n",
    "\n",
    "Much of the code was taken, from https://jmetzen.github.io/2015-11-27/vae.html. However, I tried to focus more on the mathematical understanding, not so much on design of the algorithm.\n",
    "\n",
    "### Some theoretical considerations \n",
    "\n",
    "#### Outline\n",
    "Situation: $x$ is from a high-dimensional space and $z$ is from a low-dimensional (latent) space, from which we like to reconstruct $p(x)$. \n",
    "\n",
    "We consider a parameterized model $p_\\theta(x|z)$ (with parameter $\\theta$), to construct x for a given value of $z$. We build this model: \n",
    "\n",
    "* $p_\\theta(x | z)$ with a neural network determening the parameters $\\mu, \\Sigma$ of a Gaussian (or as done here with a Bernoulli-Density). \n",
    "\n",
    "#### Inverting $p_\\theta(x | z)$\n",
    "\n",
    "The inversion is not possible, we therefore approximate $p(z|x)$ by $q_\\phi (z|x)$ again a combination of a NN determening the parameters of a Gaussian\n",
    "\n",
    "* $q_\\phi(z | x)$ with a neural network + Gaussian \n",
    "\n",
    "#### Training\n",
    "\n",
    "We train the network treating it as an autoencoder. \n",
    "\n",
    "#### Lower bound of the Log-Likelihood\n",
    "The likelihood cannot be determined analytically. Therefore, in a first step we derive a lower (variational) bound $L^{v}$ of the log likelihood, for a given image. Technically we assume a discrete latent space. For a continous case simply replace the sum by the appropriate integral over the respective densities. We replace the inaccessible conditional propability $p(z|x)$ with an approximation $q(z|x)$ for which we later use a neural network topped by a Gaussian.\n",
    "\n",
    "\\begin{align}\n",
    "L & = \\log\\left(p(x)\\right) &\\\\\n",
    "  & = \\sum_z q(z|x) \\; \\log\\left(p(x)\\right) &\\text{multiplied with 1 }\\\\\n",
    "  & = \\sum_z q(z|x) \\; \\log\\left(\\frac{p(z,x)}{p(z|x)}\\right) &\\\\\n",
    "  & = \\sum_z q(z|x) \\; \\log\\left(\\frac{p(z,x)}{q(z|x)} \\frac{q(z|x)}{p(z|x)}\\right) &\\\\\n",
    "  & = \\sum_z q(z|x) \\; \\log\\left(\\frac{p(z,x)}{q(z|x)}\\right) + \\sum_z q(z|x) \\; \\log\\left(\\frac{q(z|x)}{p(z|x)}\\right) &\\\\\n",
    "  & = L^{\\tt{v}} + D_{\\tt{KL}} \\left( q(z|x) || p(z|x) \\right) &\\\\\n",
    "  & \\ge L^{\\tt{v}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "The KL-Divergence $D_{\\tt{KL}}$ is always positive, and the smaller the better $q(z|x)$ approximates $p(z|x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewritting  $L^\\tt{v}$\n",
    "We split $L^\\tt{v}$ into two parts.\n",
    "\n",
    "\\begin{align}\n",
    "L^{\\tt{v}} & = \\sum_z q(z|x) \\; \\log\\left(\\frac{p(z,x)}{q(z|x)}\\right)  & \\text{with} \\;\\;p(z,x) = p(x|z) \\,p(z)\\\\\n",
    "  & =  \\sum_z q(z|x) \\; \\log\\left(\\frac{p(x|z) p(z)}{q(z|x)}\\right)  &\\\\\n",
    "  & =  \\sum_z q(z|x) \\; \\log\\left(\\frac{p(z)}{q(z|x)}\\right)  + \\sum_z q(z|x) \\; \\log\\left(p(x|z)\\right) &\\\\\n",
    "  & =  -D_{\\tt{KL}} \\left( q(z|x) || p(z) \\right)  +  \\mathbb{E}_{q(z|x)}\\left( \\log\\left(p(x|z)\\right)\\right) &\\text{putting in } x^{(i)} \\text{ for } x\\\\\n",
    "  & =  -D_{\\tt{KL}} \\left( q(z|x^{(i)}) || p(z) \\right)  +  \\mathbb{E}_{q(z|x^{(i)})}\\left( \\log\\left(p(x^{(i)}|z)\\right)\\right) &\\\\\n",
    "\\end{align}\n",
    "\n",
    "Approximating $\\mathbb{E}_{q(z|x^{(i)})}$ with sampling form the distribution $q(z|x^{(i)})$\n",
    "\n",
    "#### Sampling \n",
    "With $z^{(i,l)}$ $l = 1,2,\\ldots L$ sampled from $z^{(i,l)} \\thicksim q(z|x^{(i)})$\n",
    "\\begin{align}\n",
    "L^{\\tt{v}} & = -D_{\\tt{KL}} \\left( q(z|x^{(i)}) || p(z) \\right)  \n",
    "+  \\mathbb{E}_{q(z|x^{(i)})}\\left( \\log\\left(p(x^{(i)}|z)\\right)\\right) &\\\\\n",
    "L^{\\tt{v}} & \\approx -D_{\\tt{KL}} \\left( q(z|x^{(i)}) || p(z) \\right)  \n",
    "+  \\frac{1}{L} \\sum_{i=1}^L \\log\\left(p(x^{(i)}|z^{(i,l)})\\right) &\\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Calculation of $D_{\\tt{KL}} \\left( q(z|x^{(i)}) || p(z) \\right)$\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
