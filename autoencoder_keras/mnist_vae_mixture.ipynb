{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Activation, Lambda, Embedding, Reshape, RepeatVector\n",
    "from keras.layers import merge\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import backend as K\n",
    "from keras import objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from keras.layers import stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References :\n",
    "##### VAE with GMM : https://arxiv.org/pdf/1611.05148.pdf\n",
    "##### Categorical VAE parameterization : http://blog.evjang.com/2016/11/tutorial-categorical-variational.html\n",
    "##### Auto-Encoding Variational Bayes : https://arxiv.org/abs/1312.6114\n",
    "##### Tutorial from Oliver Durr : https://home.zhaw.ch/~dueo/bbs/files/vae.pdf\n",
    "##### Building autoencoders in keras : https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Variational Auto-Encoder with Gaussian Mixture generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "original_dim = 784\n",
    "latent_dim = 2\n",
    "\n",
    "NM = 10 # number of mixture clusters\n",
    "\n",
    "intermediate_dim1 = 500\n",
    "intermediate_dim2 = 500\n",
    "\n",
    "nb_epoch = 25\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparameterization trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Approximate Multinomial distrib. with a Gumbel-Softmax distrib.\n",
    "# see documentation at :\n",
    "# http://blog.evjang.com/2016/11/tutorial-categorical-variational.html\n",
    "def sampling_categorical(c_logits):\n",
    "    temperature = 0.1 # the lower it is the sharper the function is\n",
    "    eps = 1e-20\n",
    "    U = K.random_uniform(shape=(batch_size, NM), low=0, high=1)\n",
    "    gumbel_noise = -K.log(-K.log(U + eps) + eps) # sample from Gumbel(0, 1)\n",
    "    #return K.softmax((c_logits+gumbel_noise)/temperature)\n",
    "    #return (c_logits+gumbel_noise)/temperature\n",
    "    return gumbel_noise\n",
    "\n",
    "def sampling_normal(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              std=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# WARNING : gate is just the sampled c vector\\ngate = c\\n\\nerrors = merge([gate, deltasums], mode='dot')\\n##############################\\n\\n\\n\\ndef vae_loss(x, x_decoded_proba):\\n    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_proba)\\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\\n    return xent_loss + kl_loss\\n\\nvae = Model(x, x_decoded_proba)\\nvae.compile(optimizer=Adam(1e-3), loss=vae_loss)\\n\""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h1 = Dense(intermediate_dim1, activation='softplus')(x)\n",
    "h2 = Dense(intermediate_dim2, activation='softplus')(h1)\n",
    "z_mean = Dense(latent_dim, activation=None)(h2)\n",
    "z_log_var = Dense(latent_dim, activation=None)(h2)\n",
    "\n",
    "\n",
    "ONEINT = Input(batch_shape=(batch_size, 1))\n",
    "\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling_normal, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "E_mu = Embedding(NM, latent_dim, input_length=1) # WARNING : needs near-0 initialization\n",
    "E_log_var = Embedding(NM, latent_dim, input_length=1) # WARNING : needs near-1 initialization\n",
    "\n",
    "def log_pdf_normal(args): # WARNING : complete into real log(PDF(z)) function\n",
    "    # WARNING : returns log for softmax function\n",
    "    mu, log_var, z = args\n",
    "    #return K.sum( (mu - z) * (mu - z) / K.exp(log_var) )\n",
    "    # pdf = exp(-0.5 * (mu - z)^2 / exp(log_var) ) * 1/prod(sigma i) * constant\n",
    "    return K.sum(-0.5 * (mu - z) * (mu - z) / K.exp(log_var)) - 0.5 * K.sum(log_var) #+ log(constant)\n",
    "\n",
    "logpdfs = []\n",
    "for i in range(NM):\n",
    "    # batch with constant i value\n",
    "    if True:\n",
    "        dummy_d = Dense(1, weights=[np.zeros((original_dim, 1)),i*np.ones((1,))], input_dim=original_dim)\n",
    "        dummy_d.trainable = False\n",
    "        ohe_i = dummy_d(x) \n",
    "    else:\n",
    "        ohe_i = ONEINT\n",
    "    mu = Reshape((latent_dim,))( E_mu(ohe_i) )\n",
    "    log_var = Reshape((latent_dim,))( E_log_var(ohe_i) )\n",
    "    #logpdfs.append( mu )\n",
    "    logpdfs.append( Reshape((1,))( Lambda(log_pdf_normal, output_shape=(1,))([mu, log_var, z]) ) )\n",
    "\n",
    "\n",
    "logpdfmat = Reshape((NM,1))( merge(logpdfs, mode='concat', concat_axis=1) ) # .summary to check axis\n",
    "c_logits = Activation('softmax')(logpdfmat)\n",
    "# WARNING : need to compute c_logits from z\n",
    "c = Activation('softmax')( Lambda(sampling_categorical, output_shape=(NM,))(c_logits) )\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "#inputs = Input(shape=(nn_input_dim,))\n",
    "inputs = x\n",
    "#outputs = Input(shape=(8,)) # in AE model output=input=x\n",
    "outputs = x\n",
    "\n",
    "predictions = []\n",
    "for i in range(NM):\n",
    "    decoder_h1 = Dense(intermediate_dim1, activation='softplus')\n",
    "    decoder_h2 = Dense(intermediate_dim2, activation='softplus')\n",
    "    decoder_proba = Dense(original_dim, activation='sigmoid')\n",
    "    h1_decoded = decoder_h1(z)\n",
    "    h2_decoded = decoder_h2(h1_decoded)\n",
    "    x_decoded_proba = decoder_proba(h2_decoded) # a Bernoulli dist. has a single prob. parameter\n",
    "    predictions.append( x_decoded_proba )\n",
    "\n",
    "\n",
    "predmat = Reshape((NM,original_dim))( merge(predictions, mode='concat', concat_axis=1) ) #.summary to check axis\n",
    "\n",
    "deltas = merge([RepeatVector(NM)(outputs), predmat], output_shape=(NM,original_dim), mode=lambda x: -(x[0] * K.log(x[1])))\n",
    "\n",
    "deltasums = Lambda(lambda x: K.sum(x, axis=2), output_shape=lambda s: (s[0], s[1]))(deltas)# .summary to check axis\n",
    "\n",
    "hinton_trick = True # see \"Adaptive Mixtures of Local Experts\"\n",
    "if hinton_trick:\n",
    "    Hinton1 = Lambda(lambda x: K.exp(-x), output_shape=lambda s: s)\n",
    "    deltasums = Hinton1(deltasums)\n",
    "'''\n",
    "# WARNING : gate is just the sampled c vector\n",
    "gate = c\n",
    "\n",
    "errors = merge([gate, deltasums], mode='dot')\n",
    "##############################\n",
    "\n",
    "\n",
    "\n",
    "def vae_loss(x, x_decoded_proba):\n",
    "    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_proba)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae = Model(x, x_decoded_proba)\n",
    "vae.compile(optimizer=Adam(1e-3), loss=vae_loss)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12574185,  0.06108518,  0.21009727,  0.05462784,  0.04958955,\n",
       "         0.05703476,  0.0801391 ,  0.07700752,  0.01824195,  0.26643494],\n",
       "       [ 0.29096895,  0.17153522,  0.05922543,  0.02664155,  0.02271858,\n",
       "         0.04022451,  0.06268827,  0.08329213,  0.1022932 ,  0.14041215],\n",
       "       [ 0.02728114,  0.03183626,  0.03543318,  0.015043  ,  0.21386734,\n",
       "         0.07227509,  0.09050404,  0.13416098,  0.03568605,  0.34391293]], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mdl = Model([x, ONEINT], [logpdfs[0], logpdfs[1], logpdfs[2], logpdfs[3], logpdfs[4]])\n",
    "#mdl = Model([x, ONEINT], logpdfmat)\n",
    "#mdl = Model([x, ONEINT], c_logits)\n",
    "mdl = Model([x, ONEINT], c)\n",
    "#mdl = Model([x, ONEINT], predmat)\n",
    "#mdl = Model([x, ONEINT], deltas)\n",
    "#mdl = Model([x, ONEINT], deltasums)\n",
    "#mdl.summary()\n",
    "\n",
    "onearr = np.ones((batch_size, 1)).astype(int)\n",
    "#mdl.predict(input_array).shape\n",
    "np.array(mdl.predict([x_train[0:batch_size,:], onearr]))[0:3,:]\n",
    "#x_train[0:batch_size,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-168-81c2aa5a3aa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m vae.fit(x_train, x_train,\n\u001b[0m\u001b[0;32m      2\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# may have to increase this value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         validation_data=(x_test, x_test))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vae' is not defined"
     ]
    }
   ],
   "source": [
    "vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=1, # may have to increase this value\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder_mean = Model(x, z_mean)\n",
    "#encoder_stdev = Model(x, K.exp(z_log_var / 2))\n",
    "\n",
    "#encoder = Model(x, z)\n",
    "\n",
    "# build a digit generator that can sample from the learned distribution\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h1_decoded = decoder_h1(decoder_input)\n",
    "_h2_decoded = decoder_h2(_h1_decoded)\n",
    "_x_decoded_proba = decoder_proba(_h2_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
